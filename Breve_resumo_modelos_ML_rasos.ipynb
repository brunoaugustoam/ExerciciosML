{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Breve_resumo_modelos_ML_rasos.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HOP6pp_R9SpG"},"source":["# Revisão de Modelos Rasos Introdutórios em Machine Learning\n"]},{"cell_type":"markdown","metadata":{"id":"5j7TbHIzmZYC"},"source":["# Naive Bayes\n","\n","**Tarefa: Apenas um experimento para servir de baseline**"]},{"cell_type":"markdown","metadata":{"id":"FcwFU7F_u7eN"},"source":["O Naive Bayes é utilizado como um classificador, na qual a escolha da classe se baseia na probabilidade condicional de ocorrência de um evento, considerando independência dos eventos, ou seja, assume que não há correlação entre os atributos escolhidos, e por isso é chamado de \"Naive\", ingênuo. O termo \"Bayes\" vêm do Teorema de Bayes, no qual o algoritmo se baseia. É um algoritmo simples, com desempenho geralmente bom, mas não ótimo para diversas aplicações."]},{"cell_type":"markdown","metadata":{"id":"EQSxiJj4xyj4"},"source":["A probabilidade condicional de um evento A ocorrer é a probabilidade de ocorrência de A, dado que um outro evento B já ocorreu. \n","\n","Essa probabilidade é escrita na forma: \n","\n","$ p(A|B) $"]},{"cell_type":"markdown","metadata":{"id":"bd5VASI6yOfj"},"source":["O Teorema de Bayes para n elementos pode ser escrito na sua forma expandida:\n","\n","$P(y|x1, x2, ... xn) $ = $\\frac{P(y)*P(x1, x2, ... xn|y)} {P(x1, x2, ... xn)} $"]},{"cell_type":"markdown","metadata":{"id":"EVwOaKKIypPD"},"source":["Pelo fato do algoritmo assumir que os atributos são independentes entre si, a probabilidade de $xi$ dado $y$ não interefere no valor das demais variáveis e.g. $P(x(i+1)|y)$. E, como as entradas do algoritmo $(x1, x2 ... xn)$ são constantes, o teorema fica resumido a:\n","\n","$P(y|x1,x2...xn)$ <math xmlns=\"http://www.w3.org/1998/Math/MathML\"> <mstyle displaystyle=\"true\"> <mi> &#x03B1;<!--greek small letter beta--> </mi> </mstyle> </math> $P(y) * \\prod_{i=1}^n P(xi|y)$"]},{"cell_type":"markdown","metadata":{"id":"PJ8Z1acW1HPr"},"source":["Dessa forma, o algortimo do Naive Bayes selecionará o y (classe) para a qual a probabilidade de y dado os valores de X, é maximizado, ou seja:\n","\n","$ŷ = argmax_{y} P(y) * \\prod_{i=1}^n P(xi|y) $"]},{"cell_type":"markdown","metadata":{"id":"BKuHqb453Ec3"},"source":["Ao olharmos para os nossos dados, vemos que os atributos são compostos por dados contínuos, e não discretos. Dessa forma, é adequado utilizar o algoritmo \"Gaussian Naive Bayes\", já implementado na biblioteca do SciKit Learn. Vamos importá-lo abaixo\n","\n","Para esse algoritmo, $P(xi|y)$ é dada com auxílio de uma normalização dos dados, na forma: \n","\n","$P(xi|y)$ = $ \\frac{1}{\\sqrt{2πσ^2_{y}}}$ * $exp(- \\frac{(xi - μ_{y})^2} {2σ^2_{y}}) $\n","\n","Nesse contexto, a média $μ_{y}$ e o desvio padrão ${σ_{y}}$ são dados pelo valor médio e desvio padrão de $xi$ dada as observações de $y$. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"YZTwtmcdmbhk"},"source":["# Decision Tree\n","\n","**Tarefa: Variar a altura maxima da arvore (incluindo permitir altura ilimitada) e\n","mostrar os resultados graficamente**"]},{"cell_type":"markdown","metadata":{"id":"az50I5-lyBsw"},"source":["Árvores de decisão são algoritmos de aprendizado de máquina supervisionado que progressivamente diminuem o dataset em grupos menores baseados em algum atributo, idealmente, até que seja possível classificar esses conjuntos por rótulos.\n","\n","São utilizados tanto em problemas de classificação quanto de regressão. São denominadas árvores de classificação quando os dados das variáveis dependentes são categóricos (discretos, qualitativos) e árvores de regressão quando as variáveis dependentes são contínuas (numéricas, quantitativa).\n","\n","Árvores são compostas por nós, galhos e folhas. Os nós são onde estão alocados os atributos. Os galhos são gerados a partir dos nós, separando os exemplos em função de um treshold ou classe do atributo. As folhas são as saídas do algoritmo, que serão por exemplo classificados. A partir de uma folha não são gerados nós.\n","\n","A partir de cada nó, os dados são divididos de forma a aumentar a homogeneidade, ou seja, de forma a gerar grupos de dados de cada vez menos classes misturadas. \n","\n","Para esse exercício, por se tratar de uma tarefa de classificação, será utilizado um algoritmo baseado no conceito de ganho de informação para a construção da árvore. O ganho de informação é dado pela diminuição da entropia esperada ao utilizar um dado atributo. Os valores de entropia, ganho de informação e Taxa de Ganho podem ser calculados conforme abaixo"]},{"cell_type":"markdown","metadata":{"id":"dwhdbKos5Mnk"},"source":["A complexidade ou capacidade do modelo aumenta a medida que a altura da árvore aumenta. No processo de aprendizagem é importante balancear de forma que tenha atributos e nós suficientes para classificar os dados (evitar underfitting) mas sem tornar a árvore específica/complexa demais, perdendo habilidade de generalização (evitar overfitting). \n","\n","Um procedimento utilizado para evitar overfitting é chamado de *Pruning*. É uma técnica que reduz a complexidade do modelo para ganhar desempenho nos dados de treino, através da remoção de nós que estão sendo utilizados para classificar ruídos. São separados por:\n","\n","* Pré-pruning: Encerra o crescimento da árvore antecipadamente.\n","\n","* Post-pruning: Permite o crescimento completo da árvore, então remove folhas se essa remoção resultar em aumento da performance.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0m_tYUHD-Xb6"},"source":["Um Algoritmo comumente usado é o ID3 (Interative Dichotomiser), baseado no ganho de inofrmação, que segue:\n","\n","A priori\n","\n","$ Info_{D} = - \\sum_{i=1}^m{\\log_{2}pi} $ \n","\n","Entropia esperada ao utilizar um determinado atributo\n","\n","$ {Info_{A}(D)} = - \\sum_{i=1}^v{\\frac {|D|_{j}}{|D|}} * InfoD_{j}$\n","\n","Ganho de informação do atributo\n","\n","$Ganho de informação(A) = Info(d) - Info_{A}(D) $\n","\n","Onde Info(D) é a média de informação necessária para identificar a classe em D.\n","\n","$\\frac {|D|_{j}}{|D|} $ serve de peso para a $j-ésima$ partição\n","\n","$Info(D)$ é a a informação esperada que seja necessária para identificar a classe em D utilizando A em um nó.\n"]},{"cell_type":"markdown","metadata":{"id":"UMlF_-u7DPK9"},"source":["O algoritmo C4.5 é uma evoluação do ID3. Seu procedimento de separação se dá por:\n","\n","$ {SplitInfo_{A}(D)} = - \\sum_{i=1}^v{\\frac {|D|_{j}}{|D|}} $ * $log_{2} {(\\frac {|D|_{j}}{|D|})} $\n","\n","\n","onde $\\frac {|D|_{j}}{|D|} $ serve de peso para a $j-ésima$ partição\n","\n","$v$ é o número de atributos\n","\n","O Ganho de informação é dado por\n","\n","$ Razão de Ganho $ = $ \\frac {Ganho(A)}{SplitInfo_{A}(D)}  $"]},{"cell_type":"markdown","metadata":{"id":"ZOxWzi_8md5Z"},"source":["# SVM\n","\n","Avaliar os kernels linear, sigmoid, polinomial e RBF"]},{"cell_type":"markdown","metadata":{"id":"48gpUcnbPLxJ"},"source":["Support Vector Machines são algoritmos que buscam encontrar o hiperplano ótimo num espaço N-dimensional (N atributos) que separe os dados com a maior margem possível. Essa margem é contruída com base nos pontos mais próximos à fronteira de decisão (vetores suporte), igualmente distante dos pontos de cada classe. A Margem pode ser buscada admitindo diferentes níveis erro, sendo o tamanho da margem e o erro admitido parâmetros que são calibrados por um hiperparâmetro.\n","\n","O hiperplano de margem ótima incialmente seria capaz de separar apenas dados linearmente separáveis, no entanto, ao utilizar-se do \"Truque do Kernel\", os dados são virtualmente (não explicitamente) deslocados para que se tornem linearmente separáveis. O algoritmo resultante é é formalmente similar, no entanto utilizando uma função kernel não linear que transforma os dados e os torna linearmente separáveis.\n","\n","Alguns kernels classicamente utilizados são: linear, sigmoid, polinomial e RBF\n","\n","A função de perda que permite a maximização da margem é chamada *Hinge Loss*:\n","\n","$c(x,y,f(x)) = (1 - y * f(x)) $\n","\n","O custo é zero se o valor previsto e o esperado são os mesmos, e somente difere de zero se os valores não forem iguais. O parâmetro de regularização é responsável pelo *trade-off* entre a maximização da margem e a diminuição do erro. Com o fator de regularização, a Loss toma o formato:\n","\n","$ min_{w}λ ||w||^2 + \\sum_{i=1}^n(1-y_{i}<x_{i},w>)    $"]},{"cell_type":"markdown","metadata":{"id":"NDC2rSGUT4KP"},"source":["A atualização dos parâmetros é dada pelos gradientes,dados pela derivada parcial da loss com respeito aos pesos. Quando a classe predita difere da esperada, a atualização dos pesos se dá por:\n","\n","$w = w + α(y_{i} x_{i} - 2 λ w)  $\n","\n","Quando a classe predita é a mesma da esperada, a atualização dos pesos se resume a:\n","\n","$w = w - α(2 λ w)  $"]},{"cell_type":"markdown","metadata":{"id":"NoqhbwOfagY_"},"source":["Por default, o sklearn svm.SVC utiliza o parâmetro de regularização = 1. Podemos variar esse valor para obter margens mais rígidas (menos erros) ou permitir uma margem \"soft\" que permita mais erros e expanda seu tamanho.\n","\n","O kernel default é rbf. vamos inicialmente testar um kernel linear.\n","\n","O parâetro max_iter limita o número de iterações. Pode ser necessário uma vez que o tempo de execução aumenta quadraticamente com o número de amostras e pode ser impraticavel com alguns milhares de exemplos"]},{"cell_type":"markdown","metadata":{"id":"b7IoxAmURyAp"},"source":["* Linear\n","* Polinomial\n","* RBF\n","* Sigmoid\n"]},{"cell_type":"markdown","metadata":{"id":"ZBv0VfPNmfyd"},"source":["# k-NN\n","\n","Variar o numero k de vizinhos e mostrar os resultados graficamente"]},{"cell_type":"markdown","metadata":{"id":"BiARg2Kfc0Zu"},"source":["KNN - K-Nearest Neighbors ou K- vizinhos mais próximos é um algoritmo que não assume uma distribuição dos dados a priori, ou seja, é um algoritmo não paramétrico. \n","\n","Além disso, é um \"algoritmo preguiçoso\", que não requer treinamento para geração do modelo, e todo dado de treino pode ser usado em teste.\n","\n","No KNN, K é o número de vizinhos. É o hiperparâmetro mais importante. Tipicamente é um número ímpar quando utilizado numa classificação binária. \n","\n","O funcionamento do algoritmo é basicamente: Dado um ponto para o qual queremos classificar, encontre os vizinhos mais próximos e classifique o ponto com base nas classes dos vizinhos mais próximos."]},{"cell_type":"markdown","metadata":{"id":"wZ-BGyU0hARV"},"source":["KNN performa melhor com um número reduzido de atributos, principalmente com poucos dados rotulados disponíveis. Ao aumentar o número de dimensões/atributos, corre-se maior risco de provocar overfitting. "]},{"cell_type":"markdown","metadata":{"id":"FIUf9q8YhvcR"},"source":["Não há um número ótimo de vizinhos geral para quaisquer dados, vamos ter que experimentar. Para poucos vizinhos, o ruído terá grande influência no resultado, e está sujeito a menor viés e maior variância. Com muitos vizinhos, se aumenta o custo computacional, e implica em menor erro de variância, porém com maior viés."]},{"cell_type":"markdown","metadata":{"id":"oqEvj3IYmi-Z"},"source":["# Random Forest\n","\n","\n","O modelo de Random Forest opera através de blocos de árvores de decisão. Sua intuição parte de um conceito da \"Sabedoria das Multidões\". Basicamente são geradas diversas árvores de decisão simples, cada qual providenciará uma classificação para o dado, e aquela classificação com maior número de modelos predizendo, será escolhida."]},{"cell_type":"markdown","metadata":{"id":"GgcOR3lHi5r8"},"source":["Um ponto importante para o funcionamento do Random Forest é a independência/baixa correlação entre os modelos construídos. A soma das partes dos modelos simples gera um modelo mais acurado que uma única árvore mais complexa. Isso se deve ao fato das múltiplas árvores se corrigirem umas as outras de erros individuais, desde de que não errem todas na mesma direção.\n","\n","São definidos 2 pré-requisitos para o random forest:\n","* Os dados devem ser separáveis através dos atributos\n","* As predições e erros de cada árvore individual deve ter baixa correlação com as demais"]},{"cell_type":"markdown","metadata":{"id":"rDf0V_2tkvN8"},"source":["Um procedimento utilizado para garantir a não correlação dos dados é o baging. Esse método consiste em deixar que cada árvore que compõe a floresta retire amostras do dataset com reposição. Isso resulta em árvores diferentes, algumas com sobreposição de dados e outras não, garantindo árvores distintas, menos susceptíveis a erros de variância.\n","\n","Um outro procedimento adotado para garantir a geração de árvores distintas é a \"Aleatoriedade de Atributos\". Nesse processo, ao invés de escolher o melhor atributo para o nó, dentre todos os disponíveis, a árvore é forçada a escolher dentre um subconjunto aleatório de atributos, o que aumenta a chance de se obter árvores distintas.\n","\n","Dessa forma, a floresta final é composta por árvorss que não apenas foram treinadas em subconjuntos de dados distintos, mas que também utilizaram diferentes atributos para tomar as decisões."]},{"cell_type":"markdown","metadata":{"id":"Mw2Y5s1GnXo_"},"source":["Apesar de haver um pequeno incremento da acurácia de treino com mais árvores na floresta, a variação é muito pequena, e por isso vamos selecionar um modelo mais simples, com menos árvores. Além disso, pela validação cruzada se observa que as acurácias de treino estão mais distante que dois desvios padrões da acurácia média na validação cruzada, então seria uma boa prática pegar um modelo mais simples para evitar maior susceptibilidade à overfitting."]},{"cell_type":"markdown","metadata":{"id":"Ty-s5ot5mknu"},"source":["# Gradient Tree Boosting "]},{"cell_type":"markdown","metadata":{"id":"WKTbXRrNeq1t"},"source":["O Gradient Tree Boosting é um algoritmo de aprendizado de máquina que também utiliza de ensemble, ou seja, da combinação de modelos mais simples para obter uma melhor performance. A técnica de Boosting é relacionada à adição de modelos (árvores) de forma que os erros de um modelo sejam corrigidos pelos demais modelos.\n","\n","Nesse modelo, a atualização dos parâmetros é feita utilizando funções de perda aleatórias nos modelos individuais e otimização por gradiente descendente. "]}]}